#!/usr/bin/env python3
"""
æƒææ‰€æœ‰é é¢æå–æ–‡å­—å…§å®¹å’Œçµæ§‹
éµå®ˆ Apolnus å“ç‰Œä¸€è‡´æ€§è¦ç¯„
"""

import os
import re
import json
from pathlib import Path

# ç›®æ¨™é é¢åˆ—è¡¨
TARGET_PAGES = [
    'WhereToBuy',
    'ServiceCenters',
    'About',
    'FAQ',
    'Profile',
    'WarrantyRegistration',
    'SupportTicket',
    'Tickets',
    'Support',
    'PartnerProgram',
    'Careers',
    'Privacy',
    'Terms',
    'NotFound',
]

# é é¢è·¯å¾‘
PAGES_DIR = Path('/home/ubuntu/apolnus/client/src/pages')
OUTPUT_DIR = Path('/home/ubuntu/apolnus/scripts/extracted')

def extract_chinese_text(content):
    """æå–ä¸­æ–‡æ–‡å­—ï¼ˆåŒ…å«æ¨™é»ç¬¦è™Ÿï¼‰"""
    # åŒ¹é…ä¸­æ–‡å­—ç¬¦ã€æ¨™é»ç¬¦è™Ÿå’Œå¸¸è¦‹ç¬¦è™Ÿ
    pattern = r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]+'
    matches = re.findall(pattern, content)
    return matches

def extract_jsx_text(content):
    """æå– JSX ä¸­çš„æ–‡å­—å…§å®¹"""
    texts = []
    
    # æå– {t('...')} ä¸­çš„ key
    t_pattern = r"\{t\(['\"]([^'\"]+)['\"]\)\}"
    t_matches = re.findall(t_pattern, content)
    texts.extend([('t_key', key) for key in t_matches])
    
    # æå– JSX æ¨™ç±¤ä¸­çš„ç´”æ–‡å­—
    # ä¾‹å¦‚ï¼š<h1>æ–‡å­—</h1> æˆ– <p>æ–‡å­—</p>
    tag_text_pattern = r'<[^>]+>([^<{]+)</[^>]+>'
    tag_matches = re.findall(tag_text_pattern, content)
    for match in tag_matches:
        text = match.strip()
        if text and len(text) > 1:  # éæ¿¾æ‰å–®å­—ç¬¦
            texts.append(('jsx_text', text))
    
    # æå–å­—ç¬¦ä¸²å­—é¢é‡ä¸­çš„æ–‡å­—
    string_pattern = r'["\']([^"\']{3,})["\']'
    string_matches = re.findall(string_pattern, content)
    for match in string_matches:
        # éæ¿¾æ‰è·¯å¾‘ã€é¡åç­‰
        if not any(x in match for x in ['/', 'className', 'http', 'www', '.', 'px-', 'py-', 'bg-', 'text-']):
            chinese = extract_chinese_text(match)
            if chinese:
                texts.append(('string', match))
    
    return texts

def analyze_page_structure(page_name, content):
    """åˆ†æé é¢çµæ§‹ï¼Œè­˜åˆ¥æ ¸å¿ƒé—œéµå­—"""
    structure = {
        'page_name': page_name,
        'sections': [],
        'keywords': [],
        'has_form': 'form' in content.lower() or 'input' in content.lower(),
        'has_table': 'table' in content.lower() or 'thead' in content.lower(),
        'has_map': 'map' in content.lower() or 'google' in content.lower(),
    }
    
    # è­˜åˆ¥ section æ¨™ç±¤
    section_pattern = r'<section[^>]*>(.*?)</section>'
    sections = re.findall(section_pattern, content, re.DOTALL)
    structure['sections'] = [f'section_{i+1}' for i in range(len(sections))]
    
    # æ ¹æ“šé é¢åç¨±è­˜åˆ¥æ ¸å¿ƒé—œéµå­—
    keyword_map = {
        'WhereToBuy': ['è³¼è²·', 'ç¶“éŠ·å•†', 'dealer', 'buy', 'purchase'],
        'ServiceCenters': ['ç¶­ä¿®', 'æœå‹™', 'service', 'repair', 'maintenance'],
        'About': ['é—œæ–¼', 'å…¬å¸', 'about', 'company', 'mission'],
        'FAQ': ['å•é¡Œ', 'è§£ç­”', 'FAQ', 'question', 'answer'],
        'Profile': ['å€‹äºº', 'æœƒå“¡', 'profile', 'account', 'member'],
        'WarrantyRegistration': ['ä¿å›º', 'è¨»å†Š', 'warranty', 'register', 'registration'],
        'SupportTicket': ['å®¢æœ', 'å·¥å–®', 'ticket', 'support', 'help'],
        'Support': ['æ”¯æ´', 'æœå‹™', 'support', 'service', 'help'],
        'PartnerProgram': ['åˆä½œ', 'å¤¥ä¼´', 'partner', 'dealer', 'distributor'],
        'Careers': ['æ‹›è˜', 'è·ç¼º', 'career', 'job', 'recruitment'],
        'Privacy': ['éš±ç§', 'æ”¿ç­–', 'privacy', 'policy', 'data'],
        'Terms': ['æ¢æ¬¾', 'ä½¿ç”¨', 'terms', 'service', 'agreement'],
        'NotFound': ['404', 'æ‰¾ä¸åˆ°', 'not found', 'error'],
    }
    
    structure['keywords'] = keyword_map.get(page_name, [])
    
    return structure

def scan_page(page_name):
    """æƒæå–®å€‹é é¢"""
    page_file = PAGES_DIR / f'{page_name}.tsx'
    
    if not page_file.exists():
        print(f'âš ï¸  é é¢ä¸å­˜åœ¨: {page_name}')
        return None
    
    print(f'ğŸ“„ æƒæé é¢: {page_name}')
    
    with open(page_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # åˆ†æé é¢çµæ§‹
    structure = analyze_page_structure(page_name, content)
    
    # æå–æ–‡å­—å…§å®¹
    texts = extract_jsx_text(content)
    
    # æå–ä¸­æ–‡æ–‡å­—
    chinese_texts = extract_chinese_text(content)
    
    result = {
        'page_name': page_name,
        'file_path': str(page_file),
        'structure': structure,
        'texts': texts,
        'chinese_texts': list(set(chinese_texts)),  # å»é‡
        'line_count': len(content.split('\n')),
    }
    
    return result

def main():
    """ä¸»å‡½æ•¸"""
    print('ğŸš€ é–‹å§‹æƒææ‰€æœ‰é é¢...\n')
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    results = {}
    
    for page_name in TARGET_PAGES:
        result = scan_page(page_name)
        if result:
            results[page_name] = result
            print(f'   âœ… æ‰¾åˆ° {len(result["chinese_texts"])} å€‹ä¸­æ–‡æ–‡å­—ç‰‡æ®µ')
            print(f'   âœ… æ‰¾åˆ° {len(result["texts"])} å€‹æ–‡å­—å…ƒç´ ')
            print(f'   âœ… é é¢çµæ§‹: {len(result["structure"]["sections"])} å€‹ sections')
            print()
    
    # ä¿å­˜çµæœ
    output_file = OUTPUT_DIR / 'scan_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f'\nâœ… æƒæå®Œæˆï¼çµæœå·²ä¿å­˜åˆ°: {output_file}')
    print(f'ğŸ“Š ç¸½å…±æƒæäº† {len(results)} å€‹é é¢')
    
    # çµ±è¨ˆä¿¡æ¯
    total_chinese = sum(len(r['chinese_texts']) for r in results.values())
    total_texts = sum(len(r['texts']) for r in results.values())
    print(f'ğŸ“ ç¸½å…±æå–äº† {total_chinese} å€‹ä¸­æ–‡æ–‡å­—ç‰‡æ®µ')
    print(f'ğŸ“ ç¸½å…±æå–äº† {total_texts} å€‹æ–‡å­—å…ƒç´ ')

if __name__ == '__main__':
    main()
